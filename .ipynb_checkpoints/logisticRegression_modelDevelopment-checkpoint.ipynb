{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DrivenData: Heart disease prediction using Logistic Regression\n",
    "\n",
    "###### BY ANNE DE GRAAF\n",
    "\n",
    "This notebook is meant to show my train of thoughts during the development of my logistic regression model for the heart disease warm-up exercise. \n",
    "\n",
    "Prior to writing this notebook I had already evaluated the data set and applied z-score normalization to the numerical data and one-hot encoding to the categorical data (see also \"data_processing.py\" in this repository).\n",
    "\n",
    "The final score of my logistic regression model was 0.32931, which was 16th place at the time of submitting.\n",
    "\n",
    "\n",
    "[keywords: logistic regression, crossvalidation, bias, variance, k-fold, stratified k-fold, leave-p-out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from logLoss import logLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "allFeatures = pd.read_csv('../train_values_normalized.csv', index_col=0)\n",
    "y = pd.read_csv('../train_labels.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model: using 6 input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss on crossval set is: 0.55285\n",
      "after 29 iterations\n"
     ]
    }
   ],
   "source": [
    "# selection of input features to use in this model:\n",
    "useCols = ['resting_blood_pressure','age','chestPain_1',\n",
    "           'chestPain_2','chestPain_3','chestPain_4']\n",
    "X = allFeatures[useCols]\n",
    "\n",
    "# split the train set in train and crossval set\n",
    "X_train, X_cross, y_train, y_cross = train_test_split(X, y, \n",
    "                                test_size=0.3, random_state=0)\n",
    "\n",
    "# apply logistic regression and predict crossval set\n",
    "logResModel = LogisticRegression(penalty='l1', tol=1e-8, solver='liblinear').fit(X_train, \n",
    "                                                np.ravel(y_train))\n",
    "prob = logResModel.predict_proba(X_cross)\n",
    "loss = logLoss(y_cross, prob)[0,1]\n",
    "num_iter = logResModel.n_iter_\n",
    "print('log loss on crossval set is: %0.5f' %loss)\n",
    "print('after %d iterations' %num_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This log loss value is bit higher than the \"Logistic Regression Benchmark\" of 0.5381 (based on TEST set) on the competition's leader board. (high log loss value == not good)\n",
    "##### So let's perform some error analysis to see what we can improve\n",
    "\n",
    "### Comparing loss on crossval set and loss on training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss on training set is: 0.55705\n"
     ]
    }
   ],
   "source": [
    "trainProb = logResModel.predict_proba(X_train)\n",
    "trainLoss = logLoss(y_train, trainProb)[0,1]\n",
    "print('log loss on training set is: %0.5f' %trainLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error on the training set is high too, so this indicates we have a bias (underfit) problem. This was to be expected as I only used a very limited amount of features.\n",
    "\n",
    "Let's make a model with all the available input features.\n",
    "\n",
    "### Second model: using all input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss on crossval set is: 0.52440\n",
      "after 42 iterations\n",
      "log loss on training set is: 0.31098\n"
     ]
    }
   ],
   "source": [
    "# split the train set in train and crossval set\n",
    "X_train, X_cross, y_train, y_cross = train_test_split(allFeatures, y, \n",
    "                                test_size=0.3, random_state=0)\n",
    "\n",
    "# apply logistic regression and predict crossval set\n",
    "logResModel2 = LogisticRegression(penalty='l1', tol=1e-8, solver='liblinear').fit(X_train, \n",
    "                                                np.ravel(y_train))\n",
    "prob = logResModel2.predict_proba(X_cross)\n",
    "loss = logLoss(y_cross, prob)[0,1]\n",
    "num_iter = logResModel2.n_iter_\n",
    "print('log loss on crossval set is: %0.5f' %loss)\n",
    "print('after %d iterations' %num_iter)\n",
    "\n",
    "trainProb2 = logResModel2.predict_proba(X_train)\n",
    "trainLoss2 = logLoss(y_train, trainProb2)[0,1]\n",
    "print('log loss on training set is: %0.5f' %trainLoss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we evaluate these results, let's apply the model to the test set and upload the results to DrivenData to see our score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading test data\n",
    "X_test = pd.read_csv('../test_values_normalized.csv', index_col=0)\n",
    "prob_test = logResModel2.predict_proba(X_test)[:,1]\n",
    "\n",
    "# saving test results\n",
    "d = {'heart_disease_present': prob_test}\n",
    "submission_df = pd.DataFrame(data=d, index=X_test.index)\n",
    "submission_df.to_csv('../logResModel2_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log loss on the test set is: 0.3757\n",
    "\n",
    "This is quite a higher score than the \"Logistic Regression Benchmark\".\n",
    "\n",
    "However, the gap between our performance on the crossval set and the training set is hugely increased. \n",
    "This points towards a high variance problem (or overfitting). The funny thing is that our performance on the test set is a lot better however than the performance on the crossval set. This might be due to the fact that we used the simple holdout method for our crossvalidation: as our crossval set is quite small, its performance is highly dependent of which data points happen to end up in the set. So in fact, the performance on the crossval set doesn't tell us that much about the actual performance of the model on the test set (as indicated by the huge difference between the 0.3757 performance on the test set and the 0.5244 on the crossval set).\n",
    "\n",
    "To obtain a more accurate indication of our performance on the crossval set, we will now perform k-fold cross-validation. Here, we will use k=10. We will apply normal k-fold and stratified k-fold, to see the effect of a more balanced crossvalidation set on the accuracy of our prediction.\n",
    "\n",
    "### K-fold cross-validation (with k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cross-validation performance on the 10 folds:', array([1.00319694, 0.24600038, 0.39085667, 1.23460986, 0.4038797 ,\n",
      "       0.37464709, 0.40883189, 0.26932168, 0.26362185, 0.24942142]))\n",
      "mean cross-validation performance: 0.48444\n"
     ]
    }
   ],
   "source": [
    "kFold = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "X = allFeatures\n",
    "crossVal_performance = np.zeros(kFold.get_n_splits())\n",
    "i=int(0)\n",
    "for iTrain, iCross in kFold.split(X, y):\n",
    "    X_train, X_cross = X.iloc[iTrain], X.iloc[iCross]\n",
    "    y_train, y_cross = y.iloc[iTrain], y.iloc[iCross]\n",
    "    model_i = LogisticRegression(penalty='l1', tol=1e-8, solver='liblinear').fit(X_train, np.ravel(y_train))\n",
    "    prob_i = model_i.predict_proba(X_cross)\n",
    "    crossVal_performance[i] = logLoss(y_cross, prob_i)[0,1]\n",
    "    i+=1\n",
    "    \n",
    "print('cross-validation performance on the 10 folds:', crossVal_performance)\n",
    "print('mean cross-validation performance: %0.5f' %np.mean(crossVal_performance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified k-fold cross-validation (with k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cross-validation performance on the stratified 10 folds:', array([0.56053498, 0.48012009, 0.6141752 , 0.51849084, 0.14828602,\n",
      "       0.20197602, 0.22935378, 0.40995184, 0.67498695, 0.59134165]))\n",
      "mean cross-validation performance: 0.44292\n"
     ]
    }
   ],
   "source": [
    "str_kFold = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "crossVal_performance = np.zeros(str_kFold.get_n_splits())\n",
    "i=int(0)\n",
    "for iTrain, iCross in str_kFold.split(X, y):\n",
    "    X_train, X_cross = X.iloc[iTrain], X.iloc[iCross]\n",
    "    y_train, y_cross = y.iloc[iTrain], y.iloc[iCross]\n",
    "    model_i = LogisticRegression(penalty='l1', tol=1e-8, solver='liblinear').fit(X_train, np.ravel(y_train))\n",
    "    prob_i = model_i.predict_proba(X_cross)\n",
    "    crossVal_performance[i] = logLoss(y_cross, prob_i)[0,1]\n",
    "    i+=1\n",
    "    \n",
    "print('cross-validation performance on the stratified 10 folds:', crossVal_performance)\n",
    "print('mean cross-validation performance: %0.5f' %np.mean(crossVal_performance))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, our performance on the crossval set is still off from the performance on the test set. \n",
    "There is one final thing we can do, which is Leave-P-Out cross validation. \n",
    "As our training set is small (only 180 samples), we'll take p=1, also called Leave-One-Out. This is equivalent to the k-fold split with k=180.\n",
    "Also, we'll play with the LogisticRegression parameters a bit to get the lowest cross-validation estimate.\n",
    "\n",
    "### Leave-One-Out cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean cross-validation performance: 0.43205\n"
     ]
    }
   ],
   "source": [
    "leaveOneOut = KFold(n_splits=180)\n",
    "crossVal_performance = np.zeros(leaveOneOut.get_n_splits())\n",
    "i=int(0)\n",
    "for iTrain, iCross in leaveOneOut.split(X, y):\n",
    "    X_train, X_cross = X.iloc[iTrain], X.iloc[iCross]\n",
    "    y_train, y_cross = y.iloc[iTrain], y.iloc[iCross]\n",
    "    model_i = LogisticRegression(penalty='l2', tol=1e-8, solver='liblinear').fit(X_train, np.ravel(y_train))\n",
    "    prob_i = model_i.predict_proba(X_cross)\n",
    "    crossVal_performance[i] = logLoss(y_cross, prob_i)[0,1]\n",
    "    i+=1\n",
    "\n",
    "print('mean cross-validation performance: %0.5f' %np.mean(crossVal_performance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we got a little bit closer, but it seems we can't get our crossvalidation performance to get any more accurate. \n",
    "This is not due to an overfitting problem it seems, but just to the fact that we have a small data set (the test set performance was a lot closer to the training set performance).\n",
    "\n",
    "Now let's fit out model to the entire training set and submit our final test set predictions. \n",
    "\n",
    "### Fit model on entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss on training set is: 0.33812\n"
     ]
    }
   ],
   "source": [
    "# apply logistic regression on entire training set\n",
    "logResModel3 = LogisticRegression(penalty='l2', tol=1e-8, solver='liblinear').fit(allFeatures, \n",
    "                                                np.ravel(y))\n",
    "\n",
    "trainProb3 = logResModel3.predict_proba(allFeatures)\n",
    "trainLoss3 = logLoss(y, trainProb3)[0,1]\n",
    "print('log loss on training set is: %0.5f' %trainLoss3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading test data\n",
    "prob_test3 = logResModel3.predict_proba(X_test)[:,1]\n",
    "\n",
    "# saving test results\n",
    "d = {'heart_disease_present': prob_test3}\n",
    "submission_df = pd.DataFrame(data=d, index=X_test.index)\n",
    "submission_df.to_csv('../logResModel3_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final result with logistic regression: log loss on test set is: 0.32931 (16th place on the leaderboard on Oct 10, 2018!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
